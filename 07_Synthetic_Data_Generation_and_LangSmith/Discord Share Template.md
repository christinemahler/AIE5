# AIE5 Assignment 7 - Synthetic Data Generation

GitHub Link: 

Loom Video: https://www.loom.com/share/aa50b78cfc1648699310e24a014dacd4?sid=27f40dc3-0c31-4896-813b-fe8e088fe3bf

# 3 Lessons Learned

1. Good prompt engineering strategies can have a more pronounced improvement on the LLM's output than simply changing the embedding model and at a reduced cost.
2. If at first you don't succeed, try try again. In other words, developing a RAG chain (or agentic application) should be an iterative, test-driven process.
3. RAGAS is a good extensible framework with both built-in and custom metrics support for evaluating LLM workflows.

# 3 Lessons Not Learned

1. Strategies for good test dataset composition. In other words, what's a good balance of Single/Multi, Specific/Abstract evaluators?
2. Other potentially useful evaluation metrics and example applications. Are there domain-specific metrics?
3. Knowledge graph configurations. Are there different configurations of a knowledge graph that could potentially impact the quality of the test data?