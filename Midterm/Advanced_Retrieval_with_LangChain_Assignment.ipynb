{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank)).\n",
        "\n",
        "> You do not need to run the following cells if you are running this notebook locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/233.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.1/233.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.1/378.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")\n",
        "os.environ[\"RAGAS_APP_TOKEN\"] = getpass.getpass(\"Please enter your Ragas API key!\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - Advanced RAG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-01 10:03:00--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: ‘john_wick_1.csv’\n",
            "\n",
            "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-03-01 10:03:00 (5.19 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
            "\n",
            "--2025-03-01 10:03:00--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_2.csv’\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-01 10:03:01 (1.08 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
            "\n",
            "--2025-03-01 10:03:01--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_3.csv’\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-03-01 10:03:01 (3.49 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
            "\n",
            "--2025-03-01 10:03:01--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: ‘john_wick_4.csv’\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-03-01 10:03:02 (1.88 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2025, 2, 27, 20, 5, 1, 537598)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided in the context.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to seek vengeance after gangsters kill his dog and steal everything from him. This sets off a chain of events where he faces off against various enemies, including professional killers and mobsters. The movie is known for its intense action, thrilling fights, and suspenseful storyline.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Overall, opinions on John Wick vary. Some people really enjoyed the action and style of the movie, while others found it lacking substance and depth. It seems like there are mixed reviews on whether people generally liked John Wick.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for \"John Wick 4\". Here is the URL to that review: /review/rw8946038/?ref_=tt_urv'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the protagonist is a former hitman seeking vengeance for the death of his beloved dog, given to him by his deceased wife. The movie is known for its intense action sequences and the main character's skills in combat.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n- Review: A Masterpiece & Brilliant Sequel\\n- URL: /review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the protagonist, John Wick, is coerced back into the world of assassins by a mobster who blows up his house after Wick refuses to help. Wick is then tasked with killing the mobster's sister in Rome, leading to a series of events where Wick becomes the target of multiple contract killers. Eventually, Wick seeks revenge on the mobster who betrayed him.\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, people generally liked John Wick based on the reviews provided. Many reviewers praised the film for its action sequences, slickness, brutality, and Keanu Reeves' performance. Some described it as the best action film of the year and one of the best in the past decade. It was seen as a stylish, violent, and entertaining movie that exceeded expectations.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, there is a review with a rating of 10. Here is the URL to that review: '/review/rw4854296/?ref_=tt_urv'\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, a retired assassin named John Wick seeks revenge when his dog is killed and his car is stolen, leading to a lot of carnage. He is forced back into the world of assassins when he must help take over the Assassin's Guild. Wick travels to Italy, Canada, and Manhattan, killing numerous assassins along the way.\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mn/0my2dcr50j3g01czsw6gr97w0000gn/T/ipykernel_25376/3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, generally people liked John Wick.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review: /review/rw4854296/?ref_=tt_urv.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, a retired assassin named John Wick is forced back into action when someone steals his car, leading to a lot of violence and carnage. He also helps Ian McShane take over the Assassin's Guild by traveling to Italy, Canada, and Manhattan to kill many other assassins along the way.\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided in the context.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review:\\nhttps://www.imdb.com/review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In the movie \"John Wick,\" an ex-hitman comes out of retirement to seek vengeance against the gangsters who killed his dog and took everything from him. He becomes entangled in a web of violence and destruction as he becomes the target of hitmen and bounty hunters. The movie is filled with intense action, shootouts, and thrilling fights as John Wick seeks retribution.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!\n",
        "\n",
        "> NOTE: You do not need to run this cell if you're running this locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Overall, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the main character John Wick seeks revenge on the people who killed his dog and took something he loved from him. It's a simple premise for an action movie that delivers awesome action, stylish stunts, kinetic chaos, and a relatable hero.\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Wall of Imports\n",
        "\n",
        "RAG-relevant metrics include: Context Precision, Context Recall, Context Entities Recall, Noise Sensitivity, Response Relevancy, and Faithfulness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextPrecisionWithoutReference, LLMContextRecall, Faithfulness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e1ddf1c9103484dbb26fc442add0635",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e1e1c42e0b9498bb6921844c35abf6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 40b202eb-8a31-43e7-b211-7b0a9d5bea61 does not have a summary. Skipping filtering.\n",
            "Node c8b7ad1f-e40c-4eb8-9631-c769477b239c does not have a summary. Skipping filtering.\n",
            "Node e62d0b5e-f377-408d-8b6d-2ca9d8d9e641 does not have a summary. Skipping filtering.\n",
            "Node 17d1e661-ce7e-4ec4-bcb1-3478c1c83da1 does not have a summary. Skipping filtering.\n",
            "Node cf4b9820-ba03-4943-b54b-2ea781c89fac does not have a summary. Skipping filtering.\n",
            "Node c3e81d30-f249-4382-8c73-c0dc0aa23a92 does not have a summary. Skipping filtering.\n",
            "Node b35a1873-d7a0-4ae6-8228-cc6ea99a2579 does not have a summary. Skipping filtering.\n",
            "Node 32c11c40-562d-472f-ad98-962c03ec8175 does not have a summary. Skipping filtering.\n",
            "Node aa9a4e6b-0815-4baa-a0e6-b36724c46b65 does not have a summary. Skipping filtering.\n",
            "Node 8175c664-8da6-4df9-a110-625a6c330cb7 does not have a summary. Skipping filtering.\n",
            "Node 79df99b1-7d80-40ac-9f91-69700c17fd3b does not have a summary. Skipping filtering.\n",
            "Node fd071b55-7e4d-4cc7-ba39-ad3aa1d55df9 does not have a summary. Skipping filtering.\n",
            "Node 08a10de7-7ba4-4329-9c5f-e03e9fcdfc39 does not have a summary. Skipping filtering.\n",
            "Node aec103cb-7ab9-4aa7-803e-2fcce169c198 does not have a summary. Skipping filtering.\n",
            "Node 1715290c-2599-4b5b-aef7-1cff910fd742 does not have a summary. Skipping filtering.\n",
            "Node 8aee0195-8c43-4163-a771-db45146a4a87 does not have a summary. Skipping filtering.\n",
            "Node e89f6b28-e85f-4757-9ded-94ea65a88669 does not have a summary. Skipping filtering.\n",
            "Node 9149e0dc-1147-412d-a063-1b4c05249269 does not have a summary. Skipping filtering.\n",
            "Node bf2d6a87-2a13-4819-bda8-5edf6492b45b does not have a summary. Skipping filtering.\n",
            "Node 03e4b852-1cc0-422a-9c41-47b444ec5864 does not have a summary. Skipping filtering.\n",
            "Node 8e90297d-f11f-49cf-b6dc-c4e9a22e05ff does not have a summary. Skipping filtering.\n",
            "Node b274cbf7-918d-4570-8c8e-edfcaa15b5c9 does not have a summary. Skipping filtering.\n",
            "Node fcfd623a-c081-4763-bab4-7c4e216db03f does not have a summary. Skipping filtering.\n",
            "Node 10b61cc2-9965-482c-b43d-fc3c8788665c does not have a summary. Skipping filtering.\n",
            "Node b547b10f-b4fd-4e84-9783-6e26e978f0aa does not have a summary. Skipping filtering.\n",
            "Node f9b962eb-8d6d-419b-8bf9-21f2b1b5bc88 does not have a summary. Skipping filtering.\n",
            "Node 4ab1c0e2-375e-4b2d-a068-0c23229e6ae9 does not have a summary. Skipping filtering.\n",
            "Node 17af98fc-f851-44f8-807d-1f71853258d3 does not have a summary. Skipping filtering.\n",
            "Node ba5efc51-d918-470f-81e2-9897ff0534d0 does not have a summary. Skipping filtering.\n",
            "Node 4efe334a-b325-4877-924d-c735856d8c47 does not have a summary. Skipping filtering.\n",
            "Node 3e0d728f-9fa6-47bd-8fcb-c48011b0d4df does not have a summary. Skipping filtering.\n",
            "Node c8c7f5b3-bdbc-4f6e-a71e-27ec4de4a52f does not have a summary. Skipping filtering.\n",
            "Node a34837c3-1481-4ac2-9c56-124a80572d66 does not have a summary. Skipping filtering.\n",
            "Node 93ecbbba-40fc-4012-b271-9c12c69fe13a does not have a summary. Skipping filtering.\n",
            "Node aae35451-9a09-4d2e-b65c-2e178a777e37 does not have a summary. Skipping filtering.\n",
            "Node 7e59a857-8d74-42e4-a1ee-c076fb0f7748 does not have a summary. Skipping filtering.\n",
            "Node 0c629ecb-114a-40fb-b433-7d1874cb9bd0 does not have a summary. Skipping filtering.\n",
            "Node 56b4f386-8a5e-46ad-b983-2c24bf4e07f9 does not have a summary. Skipping filtering.\n",
            "Node 3d7093f3-34c1-4c66-a14b-c991497621e7 does not have a summary. Skipping filtering.\n",
            "Node 60333297-42cf-4003-b3fa-e7a5ce3b2170 does not have a summary. Skipping filtering.\n",
            "Node e7487478-d457-4fd3-980d-43cd1b86f1ed does not have a summary. Skipping filtering.\n",
            "Node f3e90820-56b3-4478-8a3e-cbd2d6801ed0 does not have a summary. Skipping filtering.\n",
            "Node 2069066a-4bd8-4a90-9f60-6b40e13522ea does not have a summary. Skipping filtering.\n",
            "Node a8c138a6-7356-42b5-ae74-12dc53b02efe does not have a summary. Skipping filtering.\n",
            "Node 119baa3b-a0cf-4567-b14c-2e60730b3461 does not have a summary. Skipping filtering.\n",
            "Node c9d0c0fb-34f4-40cf-b1bd-4d987101445a does not have a summary. Skipping filtering.\n",
            "Node 90fe3ba7-f038-4ac4-bd18-7e4c9657c64c does not have a summary. Skipping filtering.\n",
            "Node fad26589-f7ae-48ff-8541-c1446dca3ef2 does not have a summary. Skipping filtering.\n",
            "Node 68806e18-fdbb-45c5-a6bd-beb27d6bf915 does not have a summary. Skipping filtering.\n",
            "Node fee25dff-18c2-4ba4-966f-df5b9ebcd001 does not have a summary. Skipping filtering.\n",
            "Node 800693a2-6640-443c-b7ba-75ad049ec730 does not have a summary. Skipping filtering.\n",
            "Node 7e2b8e8f-3adf-4128-b611-0167447f3d8a does not have a summary. Skipping filtering.\n",
            "Node 3397f4b3-6543-4926-b0bb-5dd20f98325e does not have a summary. Skipping filtering.\n",
            "Node b7611f93-c3ac-42b6-8c70-de31e830b57e does not have a summary. Skipping filtering.\n",
            "Node 35b05322-7429-44e6-8650-b2a8f8a18a82 does not have a summary. Skipping filtering.\n",
            "Node b40b06ae-fcb3-4051-87f9-b2c7b44909a2 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6af3ba6660e45768d343172bdc2358f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/244 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12d49c9251894287bf2a345a74c44d56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c40d7ddf649b4a4c84e37dd526c26fd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a32633002ac34a54993b4d65f5e72242",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a355a888172147adb59a507f1268b289",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 19\\nReview: The inevitable third...</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 19\\nReview: John Wick: Chapter 4...</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 19\\nReview: The inevitable third...</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 13\\nReview: Following on from tw...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does the film 'John Wick' compare to 'Take...   \n",
              "1            Wht is the poplarity of Jon Wick films?   \n",
              "2  What distinguishes Chad Stahelski's direction ...   \n",
              "3  How do Russian mobsters influence the plot of ...   \n",
              "4                  What John Wick do in first movie?   \n",
              "5  What makes John Wick 3 stand out in terms of a...   \n",
              "6  How John Wick Chapter 2 and John Wick: Chapter...   \n",
              "7  How does the reception of John Wick 4 compare ...   \n",
              "8  How does the narrative coherence and character...   \n",
              "9  How does John Wick: Chapter 3 - Parabellum mai...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 19\\nReview: The inevitable third...   \n",
              "6  [<1-hop>\\n\\n: 19\\nReview: John Wick: Chapter 4...   \n",
              "7  [<1-hop>\\n\\n: 19\\nReview: The inevitable third...   \n",
              "8  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "9  [<1-hop>\\n\\n: 13\\nReview: Following on from tw...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  The film 'John Wick' can be described as simil...   \n",
              "1  The fourth installment of John Wick is scoring...   \n",
              "2  Chad Stahelski's direction in John Wick is dis...   \n",
              "3  The Russian mobsters are responsible for attac...   \n",
              "4  In the original John Wick (2014), an ex-hit-ma...   \n",
              "5  John Wick 3 stands out in terms of action sequ...   \n",
              "6  John Wick Chapter 2 is described as relentless...   \n",
              "7  The reception of John Wick 4 appears to be mix...   \n",
              "8  The narrative coherence and character developm...   \n",
              "9  John Wick: Chapter 3 - Parabellum maintains it...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(documents, testset_size=10)\n",
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = dataset.to_pandas()\n",
        "df.to_csv('testdataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naive RAG Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec5d5511c15a44f7b6de02a726eb7b9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'llm_context_precision_without_reference': 0.6183, 'context_recall': 0.7833, 'faithfulness': 0.9405, 'answer_relevancy': 0.7637, 'context_entity_recall': 0.5333, 'noise_sensitivity_relevant': 0.4585}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = naive_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "dataset.to_pandas()\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextPrecisionWithoutReference(), LLMContextRecall(), Faithfulness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BM25 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6adeedf50f984805917da418bd914642",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'llm_context_precision_without_reference': 0.3833, 'context_recall': 0.4133, 'faithfulness': 0.8000, 'answer_relevancy': 0.3933, 'context_entity_recall': 0.4100, 'noise_sensitivity_relevant': 0.4125}"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = bm25_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "dataset.to_pandas()\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextPrecisionWithoutReference(), LLMContextRecall(), Faithfulness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Contextual Compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e88665082a7045acb562b64a550798e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'llm_context_precision_without_reference': 0.8000, 'context_recall': 0.6700, 'faithfulness': 0.8564, 'answer_relevancy': 0.7654, 'context_entity_recall': 0.6517, 'noise_sensitivity_relevant': 0.1833}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = contextual_compression_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "dataset.to_pandas()\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextPrecisionWithoutReference(), LLMContextRecall(), Faithfulness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-Query Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "700f2152c14548d397c77a9766d1b32e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[59]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'llm_context_precision_without_reference': 0.5256, 'context_recall': 0.9167, 'faithfulness': 0.9033, 'answer_relevancy': 0.7632, 'context_entity_recall': 0.5083, 'noise_sensitivity_relevant': 0.4167}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = multi_query_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "dataset.to_pandas()\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextPrecisionWithoutReference(), LLMContextRecall(), Faithfulness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1b25c44298146809ca5916c27c38849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'llm_context_precision_without_reference': 0.7000, 'context_recall': 0.5517, 'faithfulness': 0.6978, 'answer_relevancy': 0.8595, 'context_entity_recall': 0.5450, 'noise_sensitivity_relevant': 0.0892}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = parent_document_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "dataset.to_pandas()\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextPrecisionWithoutReference(), LLMContextRecall(), Faithfulness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad55fa09bad44c64939eb034733abf41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[11]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'llm_context_precision_without_reference': 0.6339, 'context_recall': 0.9667, 'faithfulness': 0.7665, 'answer_relevancy': 0.8519, 'context_entity_recall': 0.6333, 'noise_sensitivity_relevant': 0.3600}"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = ensemble_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "dataset.to_pandas()\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextPrecisionWithoutReference(), LLMContextRecall(), Faithfulness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create LangSmith Dataset \n",
        "Makes it easier to compare different metrics for each pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "langsmith_client = Client()\n",
        "\n",
        "dataset_name = \"Advanced RAG\"\n",
        "\n",
        "langsmith_dataset = langsmith_client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=dataset_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  langsmith_client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Configure Evaluation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define LangSmith Evaluators\n",
        "We only need one here since we're more focused on using Ragas to do our evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naive RAG LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'elderly-condition-45' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=1dc98b4e-59e8-4a98-ad69-408c41977cf3\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f17532e26b9466785e11763eb769246",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b4e9537-744c-42c5-bbb1-2410fcb11c88: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eebd86c9-d756-45e1-8af4-49378dc4b0cd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 05b710f5-2644-4580-be36-78cd9199b4ec: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eab48d70-b002-4381-b4b5-c362bb0b9233: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 88b6c0eb-76c6-4b32-8c95-eb01f9ea1fe9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b9994a7-addb-4d3d-90f1-396f9ea50170: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 70097608-90dc-4c51-991f-6aafd2b42f7c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e66b3af8-8543-43ac-ad98-0667b2b637bd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run db45139d-8a10-4a38-8578-a7e233dbc673: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6c7776cd-61ce-400d-9e89-9e92379973f8: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content='To maintain its unique appeal while d...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.534019</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>8b4e9537-744c-42c5-bbb1-2410fcb11c88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content=\"I don't know.\" additional_kwargs={'re...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.022953</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>eebd86c9-d756-45e1-8af4-49378dc4b0cd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content='Based on the reviews provided, John W...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.419759</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>05b710f5-2644-4580-be36-78cd9199b4ec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content=\"I don't have specific information com...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.266918</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>eab48d70-b002-4381-b4b5-c362bb0b9233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content=\"John Wick 3 stands out in terms of ac...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.503118</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>88b6c0eb-76c6-4b32-8c95-eb01f9ea1fe9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content='In the first John Wick movie, John Wi...</td>\n",
              "      <td>[page_content=': 5\\nReview: Ultra-violent firs...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.865102</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>9b9994a7-addb-4d3d-90f1-396f9ea50170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content='Russian mobsters influence the plot o...</td>\n",
              "      <td>[page_content=': 18\\nReview: When the story be...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.975777</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>70097608-90dc-4c51-991f-6aafd2b42f7c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.163760</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>e66b3af8-8543-43ac-ad98-0667b2b637bd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content='The John Wick films have been consist...</td>\n",
              "      <td>[page_content=': 9\\nReview: At first glance, J...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.178945</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>db45139d-8a10-4a38-8578-a7e233dbc673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"In terms of plot, both 'John Wick' an...</td>\n",
              "      <td>[page_content=': 0\\nReview: The best way I can...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.650874</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>6c7776cd-61ce-400d-9e89-9e92379973f8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults elderly-condition-45>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    naive_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BM25 LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'stupendous-energy-43' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=955131e8-c661-4e27-adf7-4319a98bd9fc\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b670a320fe7746dfb5de71a92e772090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d8376b50-60b1-4b4d-89e5-e74037f74c07: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 90b2a6a6-fe80-4924-ac51-7df0a6423c53: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 374adad3-8a7b-4baf-a27e-ef2b82932cc5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eb14b4f8-f159-4b4e-8d46-bfea983e5add: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 20c33e8d-8748-4019-9a64-46e154579bd3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 85867c2b-2966-4f89-8009-8cbb71af49a9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 70193206-e95a-4c9a-80b5-0ba35c3e1260: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3b08298b-79e4-469f-b8ed-67b682e4366b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dd4fdd7b-4dcd-433b-9a80-525589529437: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9be22599-b1c4-423c-a05b-92f13ccdaf68: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content=\"John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.196603</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>d8376b50-60b1-4b4d-89e5-e74037f74c07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content=\"I don't know the specifics of how nar...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.653177</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>90b2a6a6-fe80-4924-ac51-7df0a6423c53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content=\"Based on the reviews provided, it see...</td>\n",
              "      <td>[page_content=': 2\\nReview: The first three Jo...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.192218</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>374adad3-8a7b-4baf-a27e-ef2b82932cc5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content='In terms of action and character deve...</td>\n",
              "      <td>[page_content=': 16\\nReview: John Wick Chapter...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.919266</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>eb14b4f8-f159-4b4e-8d46-bfea983e5add</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content='John Wick 3 stands out in terms of ac...</td>\n",
              "      <td>[page_content=': 22\\nReview: Lets contemplate ...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.686215</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>20c33e8d-8748-4019-9a64-46e154579bd3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content=\"I'm sorry, but the context provided d...</td>\n",
              "      <td>[page_content=': 11\\nReview: Who needs a 2hr a...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.519040</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>85867c2b-2966-4f89-8009-8cbb71af49a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content='I don\\'t know the specific details ab...</td>\n",
              "      <td>[page_content=': 11\\nReview: Who needs a 2hr a...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.532849</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>70193206-e95a-4c9a-80b5-0ba35c3e1260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.754211</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>3b08298b-79e4-469f-b8ed-67b682e4366b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content=\"I don't know.\" additional_kwargs={'re...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.436681</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>dd4fdd7b-4dcd-433b-9a80-525589529437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"I'm sorry, I don't have the specific ...</td>\n",
              "      <td>[page_content=': 20\\nReview: In a world where ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.802305</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>9be22599-b1c4-423c-a05b-92f13ccdaf68</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults stupendous-energy-43>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    bm25_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Contextual Compression LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'abandoned-process-81' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=24104c65-67f2-4687-ad60-3ad68f28b347\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59c424695a9f40c1ac4a80c5affff321",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8b5cd23a-dfbf-4609-a8f4-432e8f1652a3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d7489ee6-86c1-4a37-932d-d3d391e69dbc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a4559539-0989-437b-af0c-3e333bc72071: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 83e34d02-8101-4b44-a1ba-f3452f72f65c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9f2c15a9-8fd8-4127-b1c9-8349a42ca384: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c92d60aa-db65-4b2c-9c21-78d8ab5bdf53: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3c019838-e3bd-4dd6-8744-68ea96780839: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3cfd546b-040c-4bfe-85d6-83488df6c864: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8faa8ad6-19f1-428b-8ea8-3c190c38a755: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9f81bee8-6ebc-4fb6-9ae2-62ee21353e6c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content=\"John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.757350</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>8b5cd23a-dfbf-4609-a8f4-432e8f1652a3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content=\"I don't have specific information rel...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.461110</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>d7489ee6-86c1-4a37-932d-d3d391e69dbc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content=\"Based on the reviews provided, John W...</td>\n",
              "      <td>[page_content=': 20\\nReview: In a world where ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.332468</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>a4559539-0989-437b-af0c-3e333bc72071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content=\"I'm sorry, I don't have specific info...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.669970</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>83e34d02-8101-4b44-a1ba-f3452f72f65c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content='John Wick 3 stands out in terms of ac...</td>\n",
              "      <td>[page_content=': 16\\nReview: John Wick 3 is wi...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.099758</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>9f2c15a9-8fd8-4127-b1c9-8349a42ca384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content='In the first John Wick movie, John Wi...</td>\n",
              "      <td>[page_content=': 5\\nReview: Ultra-violent firs...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.075636</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>c92d60aa-db65-4b2c-9c21-78d8ab5bdf53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content='The Russian mobsters influence the pl...</td>\n",
              "      <td>[page_content=': 20\\nReview: After resolving h...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.394296</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>3c019838-e3bd-4dd6-8744-68ea96780839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.275004</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>3cfd546b-040c-4bfe-85d6-83488df6c864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content='I am sorry, but I do not have specifi...</td>\n",
              "      <td>[page_content=': 11\\nReview: JOHN WICK is a ra...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.943287</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>8faa8ad6-19f1-428b-8ea8-3c190c38a755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"Based on the provided context, both '...</td>\n",
              "      <td>[page_content=': 11\\nReview: JOHN WICK is a ra...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.929755</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>9f81bee8-6ebc-4fb6-9ae2-62ee21353e6c</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults abandoned-process-81>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    contextual_compression_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-Query LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'large-rabbit-43' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=ee6ca056-06b5-44cf-a205-8319d4a30e7d\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e09d71bb75a14e4b8c133b8e320ad55e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5c71f072-3fd5-40ee-bbce-d705627ed7c5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e2713310-b01f-4ffd-a06f-ffeb6168ca38: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 90f6cff7-60f4-45a9-a3b7-97ca9923612c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 163c5359-2054-4214-abef-5dbffd7ed948: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 46af1d3f-b83a-4787-afc3-92cd27b0f751: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cc43fd85-3488-425c-a975-4524ceb46735: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22eb3c4b-af61-4fe3-b543-705315b4d76e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6419dd0a-ddf4-4b77-8624-4b47f14bb5c6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dbf96cd3-9f4b-4a07-a079-de0b64d5c4dd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run acaf28b4-4706-4108-a4f5-45dd8d013478: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content=\"John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.378689</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>5c71f072-3fd5-40ee-bbce-d705627ed7c5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content=\"I don't know the specific details of ...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.874796</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>e2713310-b01f-4ffd-a06f-ffeb6168ca38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content='Based on the reviews provided, the re...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.740345</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>90f6cff7-60f4-45a9-a3b7-97ca9923612c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content=\"I don't have specific details compari...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.990491</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>163c5359-2054-4214-abef-5dbffd7ed948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content='John Wick 3 stands out in terms of ac...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.730219</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>46af1d3f-b83a-4787-afc3-92cd27b0f751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content='In the first John Wick movie, John Wi...</td>\n",
              "      <td>[page_content=': 19\\nReview: If you've seen th...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>5.718881</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>cc43fd85-3488-425c-a975-4524ceb46735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content='The Russian mobsters influence the pl...</td>\n",
              "      <td>[page_content=': 18\\nReview: When the story be...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>6.780662</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>22eb3c4b-af61-4fe3-b543-705315b4d76e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.507542</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>6419dd0a-ddf4-4b77-8624-4b47f14bb5c6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content='The John Wick films have been very po...</td>\n",
              "      <td>[page_content=': 20\\nReview: In a world where ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.902570</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>dbf96cd3-9f4b-4a07-a079-de0b64d5c4dd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"In terms of plot and execution, both ...</td>\n",
              "      <td>[page_content=': 0\\nReview: The best way I can...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.508383</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>acaf28b4-4706-4108-a4f5-45dd8d013478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults large-rabbit-43>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    multi_query_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'artistic-fish-1' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=d4e31f91-048a-4f63-8311-7c08e5016d5d\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b349dd1fdf6c4c2baa403e36b9d2408d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 015c0bd7-30de-4e9e-91be-1569a3246d17: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9f7d13d5-e705-4040-a4cb-42c6cdba38cd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cb65b930-acef-4cba-8441-6414f169efa7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22fb3206-3578-4b59-893f-8e500312fc07: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bff4392c-6351-459c-ab1e-cbea66173f12: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bf165d6b-9c8f-45e7-b37b-e4e3bc2c22fd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 63e48d3f-6209-4a94-8bf2-83ee51a501eb: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 97729ce6-ecb3-4066-beb0-9fe2e89f4d54: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7c4f41bc-3325-4ff7-8662-ab4c3df74f72: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3b982289-875c-4b23-b305-f37a3458175a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content=\"John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.370694</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>015c0bd7-30de-4e9e-91be-1569a3246d17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content='The narrative coherence and character...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.477476</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>9f7d13d5-e705-4040-a4cb-42c6cdba38cd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content='Based on the reviews provided, the re...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.205801</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>cb65b930-acef-4cba-8441-6414f169efa7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content=\"Based on the provided context, John W...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.478003</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>22fb3206-3578-4b59-893f-8e500312fc07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content='In John Wick 3, what makes it stand o...</td>\n",
              "      <td>[page_content=': 1\\nReview: I'm a fan of the J...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.885871</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>bff4392c-6351-459c-ab1e-cbea66173f12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content='In the first John Wick movie, John Wi...</td>\n",
              "      <td>[page_content=': 5\\nReview: Ultra-violent firs...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.127982</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>bf165d6b-9c8f-45e7-b37b-e4e3bc2c22fd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content=\"Russian mobsters influence the plot o...</td>\n",
              "      <td>[page_content=': 20\\nReview: John Wick is some...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.998202</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>63e48d3f-6209-4a94-8bf2-83ee51a501eb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content=': 18\\nReview: Ever since the or...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.990364</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>97729ce6-ecb3-4066-beb0-9fe2e89f4d54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content='The John Wick films seem to be quite ...</td>\n",
              "      <td>[page_content=': 20\\nReview: In a world where ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.997374</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>7c4f41bc-3325-4ff7-8662-ab4c3df74f72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"Based on the provided context, 'John ...</td>\n",
              "      <td>[page_content=': 11\\nReview: JOHN WICK is a ra...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.241361</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>3b982289-875c-4b23-b305-f37a3458175a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults artistic-fish-1>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    parent_document_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'excellent-copy-13' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=7972a8e7-fbbb-422b-8812-a9355e310bab\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e51d99d1deb4d979a2d7a773c2ecd56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fe455257-895a-49b1-a6c0-8660c994f568: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b1cd79b7-3404-4d20-a298-d588432e15c3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b696dabc-852b-4971-86d5-e231796df42b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ce205817-f40d-4f60-af21-2deffdd90b98: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7318c71c-4206-441b-9314-6366e951a8ff: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2036a12f-437c-4ee4-9803-06692e7cbe91: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9b41edf4-9bca-4347-a792-f3e11525059f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2b5c03d1-9aee-433a-a6e4-edb287aa75b6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 845d6b2d-dec2-48be-83ae-7c0400768861: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f14552af-1cbe-4539-8731-fb2810f69ffd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content=\"John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.896029</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>fe455257-895a-49b1-a6c0-8660c994f568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content=\"I don't have the specific information...</td>\n",
              "      <td>[page_content=': 19\\nReview: The inevitable th...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.280386</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>b1cd79b7-3404-4d20-a298-d588432e15c3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content='Based on the reviews provided, the re...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>4.467646</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>b696dabc-852b-4971-86d5-e231796df42b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content=\"I'm sorry, but I don't have enough in...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.345671</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>ce205817-f40d-4f60-af21-2deffdd90b98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content=\"In John Wick 3, the action sequences ...</td>\n",
              "      <td>[page_content=': 1\\nReview: I'm a fan of the J...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>9.599096</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>7318c71c-4206-441b-9314-6366e951a8ff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content='John Wick, in the first movie, is an ...</td>\n",
              "      <td>[page_content=': 5\\nReview: Ultra-violent firs...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.530078</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>2036a12f-437c-4ee4-9803-06692e7cbe91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content=\"Russian mobsters influence the plot o...</td>\n",
              "      <td>[page_content=': 18\\nReview: When the story be...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>4.819144</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>9b41edf4-9bca-4347-a792-f3e11525059f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content=': 3\\nReview: John wick has a ve...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.323437</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>2b5c03d1-9aee-433a-a6e4-edb287aa75b6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content='The John Wick films, particularly the...</td>\n",
              "      <td>[page_content=': 20\\nReview: In a world where ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.366329</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>845d6b2d-dec2-48be-83ae-7c0400768861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"The film 'John Wick' has been compare...</td>\n",
              "      <td>[page_content=': 0\\nReview: The best way I can...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>3.982251</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>f14552af-1cbe-4539-8731-fb2810f69ffd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults excellent-copy-13>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    ensemble_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Semantic LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'dependable-test-84' at:\n",
            "https://smith.langchain.com/o/dda31471-d9b4-4423-9065-04bbf7ece062/datasets/4e3bb4ac-212c-44a7-9972-b1dd7fb0b3c0/compare?selectedSessions=c66c721c-70aa-4b7d-9d02-1039fb5489b0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77f25db76cb442058e8389223bf3f4fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 77522441-55ac-4726-91f3-08393036fbba: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6772f933-8d02-44f9-8b09-4ddd91743c76: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2c1f0bec-e292-420e-848d-43075b1f2f7f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f9c97ac-0f44-467e-aba3-9ab77d2f3f1e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 56c261cb-a3d4-4e93-9994-d7f99dc73542: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 288d713f-83f0-4cb0-9e74-f4275461f942: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ce93eace-9655-4ccd-b5d6-e24e876d525f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 737760f2-7bc4-452a-bdf3-b234380c5269: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d1e4baf1-d097-4f3e-9d02-19dfa1041866: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5da22807-3629-44d5-b7fe-8524d9b4aeb6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name=\\'gpt-4o\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/christinemahler/Desktop/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x35025dd10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x35025df90>, root_client=<openai.OpenAI object at 0x35025dbd0>, root_async_client=<openai.AsyncOpenAI object at 0x35025de50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.response</th>\n",
              "      <th>outputs.context</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.wrapper</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>content=\"John Wick: Chapter 3 - Parabellum mai...</td>\n",
              "      <td>[page_content=': 24\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum maintains it...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.077527</td>\n",
              "      <td>cee8e8d5-f8cb-4cbd-be93-92d47cd69559</td>\n",
              "      <td>77522441-55ac-4726-91f3-08393036fbba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the narrative coherence and character...</td>\n",
              "      <td>content='The narrative coherence and character...</td>\n",
              "      <td>[page_content='The story goes deeper and the a...</td>\n",
              "      <td>None</td>\n",
              "      <td>The narrative coherence and character developm...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.490263</td>\n",
              "      <td>968140bc-8ade-44f3-9fb9-c5db51a7231d</td>\n",
              "      <td>6772f933-8d02-44f9-8b09-4ddd91743c76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the reception of John Wick 4 compare ...</td>\n",
              "      <td>content='Based on the reviews provided, the re...</td>\n",
              "      <td>[page_content=': 18\\nReview: Ever since the or...</td>\n",
              "      <td>None</td>\n",
              "      <td>The reception of John Wick 4 appears to be mix...</td>\n",
              "      <td>None</td>\n",
              "      <td>2.002714</td>\n",
              "      <td>e3f2020b-816b-4353-a457-75df03eb847f</td>\n",
              "      <td>2c1f0bec-e292-420e-848d-43075b1f2f7f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How John Wick Chapter 2 and John Wick: Chapter...</td>\n",
              "      <td>content=\"I don't know the specific details abo...</td>\n",
              "      <td>[page_content=': 19\\nReview: John Wick: Chapte...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick Chapter 2 is described as relentless...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.525434</td>\n",
              "      <td>7321dad7-d531-419b-a310-4d97440d454e</td>\n",
              "      <td>6f9c97ac-0f44-467e-aba3-9ab77d2f3f1e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes John Wick 3 stand out in terms of a...</td>\n",
              "      <td>content='John Wick 3 stands out in terms of ac...</td>\n",
              "      <td>[page_content='This is EXACTLY what you want o...</td>\n",
              "      <td>None</td>\n",
              "      <td>John Wick 3 stands out in terms of action sequ...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.409272</td>\n",
              "      <td>428145ee-3e0d-4322-b7bc-f0f0df7cd735</td>\n",
              "      <td>56c261cb-a3d4-4e93-9994-d7f99dc73542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What John Wick do in first movie?</td>\n",
              "      <td>content='John Wick seeks revenge on the gangst...</td>\n",
              "      <td>[page_content='John Wick (Reeves) is out to se...</td>\n",
              "      <td>None</td>\n",
              "      <td>In the original John Wick (2014), an ex-hit-ma...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.805870</td>\n",
              "      <td>7e8f48db-1869-4bbd-aa76-53c66f548378</td>\n",
              "      <td>288d713f-83f0-4cb0-9e74-f4275461f942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do Russian mobsters influence the plot of ...</td>\n",
              "      <td>content='The Russian mobsters influence the pl...</td>\n",
              "      <td>[page_content='One day, he's getting gas and a...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Russian mobsters are responsible for attac...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.868326</td>\n",
              "      <td>0ee076bc-3637-4aa1-9503-f28698333c17</td>\n",
              "      <td>ce93eace-9655-4ccd-b5d6-e24e876d525f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What distinguishes Chad Stahelski's direction ...</td>\n",
              "      <td>content=\"Chad Stahelski's direction in John Wi...</td>\n",
              "      <td>[page_content='Directed by Chad Stahelski who'...</td>\n",
              "      <td>None</td>\n",
              "      <td>Chad Stahelski's direction in John Wick is dis...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.345579</td>\n",
              "      <td>8947bfc1-e67d-48d3-ae98-dfe1410c911a</td>\n",
              "      <td>737760f2-7bc4-452a-bdf3-b234380c5269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wht is the poplarity of Jon Wick films?</td>\n",
              "      <td>content=\"I don't know.\" additional_kwargs={'re...</td>\n",
              "      <td>[page_content='John Wick was cool.' metadata={...</td>\n",
              "      <td>None</td>\n",
              "      <td>The fourth installment of John Wick is scoring...</td>\n",
              "      <td>None</td>\n",
              "      <td>0.813832</td>\n",
              "      <td>69319b0b-968f-488b-94c2-808351d509a2</td>\n",
              "      <td>d1e4baf1-d097-4f3e-9d02-19dfa1041866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the film 'John Wick' compare to 'Take...</td>\n",
              "      <td>content=\"The film 'John Wick' has been describ...</td>\n",
              "      <td>[page_content='John Wick (Reeves) is out to se...</td>\n",
              "      <td>None</td>\n",
              "      <td>The film 'John Wick' can be described as simil...</td>\n",
              "      <td>None</td>\n",
              "      <td>1.133374</td>\n",
              "      <td>94ff1162-6c93-4901-bbe2-cf5e2948db4c</td>\n",
              "      <td>5da22807-3629-44d5-b7fe-8524d9b4aeb6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults dependable-test-84>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    semantic_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation Results\n",
        "\n",
        "Based on the table below and excluding the Semantic chunking approach for the time being, it appears you can roughly rank each retriever as follows:\n",
        "\n",
        "- **Lowest Cost:** Parent Document\n",
        "- **Fastest:** BM25\n",
        "- **Best Context Precision:** Contextual Compression\n",
        "- **Best Context Recall:** Ensemble\n",
        "- **Most Faithful:** Naive RAG\n",
        "- **Most Relevant:** Parent Document\n",
        "- **Best Context Entity Recall:** Contextual Compression\n",
        "- **Least Noise Sensitive:** Parent Document\n",
        "- **Best Performance (Averaging Across First 5 Metrics):** Ensemble\n",
        "- **Most Balanced (Between Latency, Cost, and Performance):** Contextual Compression or Parent Document\n",
        "\n",
        "Before deciding on any particular solution though, it would probably be a good idea to first rank the metrics in terms of importance for your project in addition to some kind of tolerance level. For example, would it be ok to sacrifice some precision for lower cost? Or some latency for better faithfulness? Based on the results, it appears there will always be some trade off. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "| Retriever Type | Avg Latency | Cost | Context Precision | Context Recall | Faithfulness | Response Relevancy | Context Entity Recall | Average | Noise Sensitivity |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| Naive RAG | 1.458 | Total: $0.02, Avg: $0.002 | 0.6183 | 0.7833 | 0.9405 | 0.7637 | 0.5333 | 0.72782 | 0.4585 |\n",
        "| BM25 | 0.869 | Total: $0.01, Avg: $0.001 | 0.3833 | 0.4133 | 0.8000 | 0.3933 | 0.4100 | 0.47998 | 0.4125 |\n",
        "| Contextual Compression | 1.594 | Total: $0.01, Avg: $0.001 | 0.8000 | 0.6700 | 0.8564 | 0.7654 | 0.6517 | 0.7487 | 0.1833 |\n",
        "| Multi-Query | 3.814 | Total: $0.024, Avg: $0.0024 | 0.5256 | 0.9167 | 0.9033 | 0.7632 | 0.5083 | 0.72342 | 0.4167 |\n",
        "| Parent Document | 1.279 | Total: $0.001, Avg: $0.0001 | 0.7000 | 0.5517 | 0.6978 | 0.8595 | 0.5450 | 0.6708 | 0.0892 |\n",
        "| Ensemble | 4.362 | Total: $0.029, Avg: $0.0029 | 0.6339 | 0.9667 | 0.7665 | 0.8519 | 0.6333 | 0.77046 | 0.3600 |\n",
        "| Semantic | 1.246 | Total: $0.016, Avg: $0.0016 |  |  |  |  |  | |  |"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
