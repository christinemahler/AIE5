# AIE5 Assignment 10 - Unsloth GRPO Training

GitHub Link: https://github.com/christinemahler/AIE5/blob/main/10_Finetuning_Reasoning_Model/AI_Makerspace_Unsloth_GRPO_Training%20COMPLETED.ipynb

Loom Video: https://www.loom.com/share/d1730d018f9a46779e3254e0670f2eb5?sid=dd3936f5-e980-4748-920c-6d5265c99d58

# 3 Lessons Learned

1. It's important to pin versions.
2. Fine-tuning is resource-intensive process that is greatly reduced through the use of LoRA and QLoRA techniques.
3. GRPO is a more efficient reinforcement learning algorithm that leverages group averaging of reward functions to autonomously either reward or penalize model responses for different training scenarios. 

# 3 Lessons Not Learned

1. Alternatives to Unsloth or any of the other fine-tuning tools used in this session.
2. Is it always appropriate to use QLoRA? Are there specific use cases where either PEFT or LoRA are better?
3. What are the risks to strictly RL fine-tuning? How may these be mitigated?