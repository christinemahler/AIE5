# AIE5 Assignment 7 - Synthetic Data Generation

GitHub Link: https://github.com/christinemahler/AIE5/blob/main/07_Synthetic_Data_Generation_and_LangSmith/Synthetic_Data_Generation_RAGAS_%26_LangSmith_Assignment.ipynb

Loom Video: https://www.loom.com/share/2d1e9e5b4a814492989f131b9cf4ce6a?sid=cae1548e-67ed-4cb0-9da7-61e5b100d8ea

# 3 Lessons Learned

1. Good prompt engineering strategies can have a more pronounced improvement on the LLM's output than simply changing the embedding model and at a reduced cost.
2. If at first you don't succeed, try try again. In other words, developing a RAG chain (or agentic application) should be an iterative, test-driven process.
3. RAGAS is a good extensible framework with both built-in and custom metrics support for evaluating LLM workflows.

# 3 Lessons Not Learned

1. Strategies for good test dataset composition. In other words, what's a good balance of Single/Multi, Specific/Abstract evaluators?
2. Other potentially useful evaluation metrics and example applications. Are there domain-specific metrics?
3. Knowledge graph configurations. Are there different configurations of a knowledge graph that could potentially impact the quality of the test data?